{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing needed libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport time\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Subset\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:19.030458Z","iopub.execute_input":"2023-12-01T19:06:19.031541Z","iopub.status.idle":"2023-12-01T19:06:25.506227Z","shell.execute_reply.started":"2023-12-01T19:06:19.031492Z","shell.execute_reply":"2023-12-01T19:06:25.505418Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing needed libraries","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n])\n\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:25.507995Z","iopub.execute_input":"2023-12-01T19:06:25.508414Z","iopub.status.idle":"2023-12-01T19:06:33.732363Z","shell.execute_reply.started":"2023-12-01T19:06:25.508387Z","shell.execute_reply":"2023-12-01T19:06:33.731559Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:03<00:00, 45455953.42it/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GoogLeNet Model ","metadata":{}},{"cell_type":"code","source":"class Inception(nn.Module):\n    \n    def __init__(self, in_channels=3, use_auxiliary=True, num_classes=1000):\n        super(Inception, self).__init__()\n        \n        self.conv1 = ConvBlock(in_channels, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.conv2 = ConvBlock(64, 192, kernel_size=3, stride=1, padding=1)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.inception3a = InceptionBlock(192, 64, 96, 128, 16, 32, 32)\n        self.inception3b = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.inception4a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n        self.inception4b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n        self.inception4c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n        self.inception4d = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n        self.auxiliary4a = Auxiliary(512, num_classes)\n        \n        self.inception4e = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.inception5a = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n        self.inception5b = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n        \n        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n        self.dropout = nn.Dropout(0.4)\n        self.linear = nn.Linear(1024, num_classes)\n        \n        self.use_auxiliary = use_auxiliary\n        if use_auxiliary:\n            self.auxiliary4d = Auxiliary(528, num_classes)\n\n    def forward(self, x):\n        auxiliary_outputs = []\n\n        x = self.conv1(x)\n        x = self.maxpool1(x)\n        x = self.conv2(x)\n        x = self.maxpool2(x)\n        \n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        \n        x = self.inception4a(x)\n        auxiliary_outputs.append(self.auxiliary4a(x))\n        \n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        if self.use_auxiliary:\n            auxiliary_outputs.append(self.auxiliary4d(x))\n        \n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        \n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        \n        x = self.avgpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.dropout(x)\n        \n        x = self.linear(x)\n        x = F.softmax(x, dim=1)\n        if self.use_auxiliary:\n            return x, auxiliary_outputs\n        else:\n            return x\n        \n\n\nclass ConvBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n    \n    \n    \n\nclass InceptionBlock(nn.Module):\n    \n    def __init__(self, im_channels, num_1x1, num_3x3_red, num_3x3, num_5x5_red, num_5x5, num_pool_proj):\n        super(InceptionBlock, self).__init__()\n        \n        self.one_by_one = ConvBlock(im_channels, num_1x1, kernel_size=1)\n        \n        self.tree_by_three_red = ConvBlock(im_channels, num_3x3_red, kernel_size=1)  \n        self.tree_by_three = ConvBlock(num_3x3_red, num_3x3, kernel_size=3, padding=1)\n        \n        self.five_by_five_red = ConvBlock(im_channels, num_5x5_red, kernel_size=1)\n        self.five_by_five = ConvBlock(num_5x5_red, num_5x5, kernel_size=5, padding=2)\n        \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.pool_proj = ConvBlock(im_channels, num_pool_proj, kernel_size=1)\n         \n    def forward(self, x):\n        x1 = self.one_by_one(x)\n        \n        x2 = self.tree_by_three_red(x)\n        x2 = self.tree_by_three(x2)\n        \n        x3 = self.five_by_five_red(x)\n        x3 = self.five_by_five(x3)\n        \n        x4 = self.maxpool(x)\n        x4 = self.pool_proj(x4)\n        \n        x = torch.cat([x1, x2, x3, x4], 1)\n        return x\n    \n    \n    \nclass Auxiliary(nn.Module):\n    \n    def __init__(self, in_channels, num_classes):\n        super(Auxiliary, self).__init__()\n        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n        self.conv1x1 = ConvBlock(in_channels, 128, kernel_size=1)\n        \n        self.fc1 = nn.Linear(2048, 1024)\n        self.fc2 = nn.Linear(1024, num_classes)\n        \n        self.dropout = nn.Dropout(0.7)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.avgpool(x)\n        x = self.conv1x1(x)\n        x = x.reshape(x.shape[0], -1)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = F.softmax(x, dim=1)\n        return x\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:33.733663Z","iopub.execute_input":"2023-12-01T19:06:33.733968Z","iopub.status.idle":"2023-12-01T19:06:33.763381Z","shell.execute_reply.started":"2023-12-01T19:06:33.733941Z","shell.execute_reply":"2023-12-01T19:06:33.762502Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:33.765566Z","iopub.execute_input":"2023-12-01T19:06:33.765944Z","iopub.status.idle":"2023-12-01T19:06:33.847318Z","shell.execute_reply.started":"2023-12-01T19:06:33.765906Z","shell.execute_reply":"2023-12-01T19:06:33.846348Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"num_classes = 10\n\n# Instantiate the model\nmodel = Inception(in_channels=3, num_classes=num_classes).to(device)\n\n\n# Define the data loaders\ndataloaders = {\"train\": train_loader, \"val\": test_loader}\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Define the scheduler\nscheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:33.848800Z","iopub.execute_input":"2023-12-01T19:06:33.849266Z","iopub.status.idle":"2023-12-01T19:06:38.604790Z","shell.execute_reply.started":"2023-12-01T19:06:33.849214Z","shell.execute_reply":"2023-12-01T19:06:38.603926Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, lr_scheduler, num_epochs=5, use_auxiliary=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    since = time.time()\n    val_acc = []\n    train_loss = []\n    train_acc = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Use tqdm for a progress bar\n            data_loader = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}')\n            for inputs, labels in data_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    if use_auxiliary:\n                        outputs, aux_outs = model(inputs)\n                        loss = criterion(outputs, labels) + 0.3 * criterion(aux_outs[0], labels) + 0.3 * criterion(aux_outs[1], labels)\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                        batch_loss = loss.item()\n                        batch_acc = torch.sum(preds == labels.data).double() / len(labels)\n\n                        train_loss.append(batch_loss)\n                        train_acc.append(batch_acc)\n\n                        data_loader.set_postfix({'Train Loss': batch_loss, 'Train Acc': batch_acc.item()})\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            if phase == 'val':\n                lr_scheduler.step(epoch_loss)\n                val_acc.append(epoch_acc)\n\n            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n\n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n\n    return model, val_acc, train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:38.605875Z","iopub.execute_input":"2023-12-01T19:06:38.606152Z","iopub.status.idle":"2023-12-01T19:06:38.621701Z","shell.execute_reply.started":"2023-12-01T19:06:38.606127Z","shell.execute_reply":"2023-12-01T19:06:38.620751Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model, val_acc, train_loss, train_acc = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T19:06:38.622819Z","iopub.execute_input":"2023-12-01T19:06:38.623105Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 0: 100%|██████████| 782/782 [04:29<00:00,  2.90it/s, Train Loss=3.46, Train Acc=0.375] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.5627 Acc: 0.2586\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 0: 100%|██████████| 157/157 [00:23<00:00,  6.57it/s]\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.6698 Acc: 0.1164\n\nEpoch 1/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|██████████| 782/782 [04:24<00:00,  2.96it/s, Train Loss=3.21, Train Acc=0.438]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.3378 Acc: 0.4494\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 1: 100%|██████████| 157/157 [00:23<00:00,  6.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.6400 Acc: 0.1635\n\nEpoch 2/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|██████████| 782/782 [04:23<00:00,  2.97it/s, Train Loss=3.17, Train Acc=0.562]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.1845 Acc: 0.5423\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 2: 100%|██████████| 157/157 [00:23<00:00,  6.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.5888 Acc: 0.2189\n\nEpoch 3/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|██████████| 782/782 [04:23<00:00,  2.97it/s, Train Loss=3.2, Train Acc=0.438] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.0912 Acc: 0.5825\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 3: 100%|██████████| 157/157 [00:24<00:00,  6.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.6216 Acc: 0.1819\n\nEpoch 4/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|██████████| 782/782 [04:22<00:00,  2.98it/s, Train Loss=3.3, Train Acc=0.375] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.0142 Acc: 0.6259\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 4: 100%|██████████| 157/157 [00:23<00:00,  6.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.5776 Acc: 0.2100\n\nEpoch 5/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|██████████| 782/782 [04:22<00:00,  2.98it/s, Train Loss=3.07, Train Acc=0.688]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.9426 Acc: 0.6767\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 5: 100%|██████████| 157/157 [00:23<00:00,  6.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.5525 Acc: 0.2265\n\nEpoch 6/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|██████████| 782/782 [04:22<00:00,  2.98it/s, Train Loss=2.84, Train Acc=0.75] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8831 Acc: 0.7152\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 6: 100%|██████████| 157/157 [00:24<00:00,  6.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.5250 Acc: 0.2458\n\nEpoch 7/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|██████████| 782/782 [04:21<00:00,  2.99it/s, Train Loss=2.9, Train Acc=0.625] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.8352 Acc: 0.7458\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 7: 100%|██████████| 157/157 [00:23<00:00,  6.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.4936 Acc: 0.2644\n\nEpoch 8/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=2.78, Train Acc=0.812]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7903 Acc: 0.7754\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 8: 100%|██████████| 157/157 [00:23<00:00,  6.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.5584 Acc: 0.2088\n\nEpoch 9/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 9: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=3.09, Train Acc=0.5]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7489 Acc: 0.7984\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 9: 100%|██████████| 157/157 [00:23<00:00,  6.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.3851 Acc: 0.3651\n\nEpoch 10/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 10: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=2.95, Train Acc=0.625]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7132 Acc: 0.8179\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 10: 100%|██████████| 157/157 [00:23<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.4173 Acc: 0.3204\n\nEpoch 11/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 11: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=2.82, Train Acc=0.688]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6847 Acc: 0.8315\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 11: 100%|██████████| 157/157 [00:23<00:00,  6.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.4602 Acc: 0.2862\n\nEpoch 12/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 12: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=2.48, Train Acc=1]    \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6527 Acc: 0.8507\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 12: 100%|██████████| 157/157 [00:23<00:00,  6.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.4304 Acc: 0.3181\n\nEpoch 13/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 13: 100%|██████████| 782/782 [04:20<00:00,  3.00it/s, Train Loss=2.77, Train Acc=0.75] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6305 Acc: 0.8618\n","output_type":"stream"},{"name":"stderr","text":"Val Epoch 13: 100%|██████████| 157/157 [00:23<00:00,  6.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 3.3746 Acc: 0.3476\n\nEpoch 14/19\n----------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 14:  82%|████████▏ | 643/782 [03:34<00:46,  3.00it/s, Train Loss=2.58, Train Acc=0.859]","output_type":"stream"}]},{"cell_type":"code","source":"val_acc= np.array([acc.cpu().numpy() for acc in val_acc])\ntrain_acc = np.array([acc.cpu().numpy() for acc in train_acc])\ntrain_loss_epc = [sum(train_loss[i:i+782]) / 782 for i in range(0, len(train_loss), 782)]\ntrain_acc_epc = [sum(train_acc[i:i+782]) / 782 for i in range(0, len(train_acc), 782)]\nplt.figure(figsize=(12, 4))\n\n# Plot Validation Accuracy\nplt.subplot(1, 3, 1)\nplt.plot((val_acc), label='Validation Accuracy', marker='o')\nplt.title('Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot Training Loss\nplt.subplot(1, 3, 2)\nplt.plot(np.array(train_loss_epc), label='Training Loss', marker='o')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot Training Accuracy\nplt.subplot(1, 3, 3)\nplt.plot((train_acc_epc), label='Training Accuracy', marker='o')\nplt.title('Training Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ResNet Model","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvBlock(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1)\n        self.conv2 = ConvBlock(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n        self.downsample = downsample\n        self.relu = nn.ReLU()\n        self.out_channels = out_channels\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out = out + residual\n        out = self.relu(out)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet34(nn.Module):\n    def __init__(self, block, layers, num_classes = 10):\n        super(ResNet34, self).__init__()\n        self.inplanes = 64\n        self.conv1 = ConvBlock(3, 64, kernel_size = 7, stride = 2, padding = 3)\n        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512, num_classes)\n        \n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(planes),)\n            \n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    \n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResNet34(ResidualBlock, [3, 4, 6, 3]).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_resnet34(model, dataloaders, criterion, optimizer, scheduler, num_epochs=5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    since = time.time()\n    val_acc = []\n    train_loss = []\n    train_acc = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Use tqdm for a progress bar\n            data_loader = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}')\n            for inputs, labels in data_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                        batch_loss = loss.item()\n                        batch_acc = torch.sum(preds == labels.data).double() / len(labels)\n\n                        train_loss.append(batch_loss)\n                        train_acc.append(batch_acc)\n\n                        data_loader.set_postfix({'Train Loss': batch_loss, 'Train Acc': batch_acc.item()})\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            if phase == 'val':\n                scheduler.step(epoch_loss)\n                val_acc.append(epoch_acc)\n\n            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n                # Save the best model\n                torch.save(model.state_dict(), 'best_resnet34.pth')\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n\n    model.load_state_dict(best_model_wts)\n    return model, val_acc, train_loss, train_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, val_acc, train_loss, train_acc = train_resnet34(model, trainloader, criterion, optimizer, scheduler, num_epochs=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}